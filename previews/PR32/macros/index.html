<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Macros · PerfTest.jl</title><meta name="title" content="Macros · PerfTest.jl"/><meta property="og:title" content="Macros · PerfTest.jl"/><meta property="twitter:title" content="Macros · PerfTest.jl"/><meta name="description" content="Documentation for PerfTest.jl."/><meta property="og:description" content="Documentation for PerfTest.jl."/><meta property="twitter:description" content="Documentation for PerfTest.jl."/><meta property="og:url" content="https://JuliaPerf.github.io/PerfTest.jl/macros/"/><meta property="twitter:url" content="https://JuliaPerf.github.io/PerfTest.jl/macros/"/><link rel="canonical" href="https://JuliaPerf.github.io/PerfTest.jl/macros/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">PerfTest.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li class="is-active"><a class="tocitem" href>Macros</a><ul class="internal"><li><a class="tocitem" href="#Declaring-test-targets"><span>Declaring test targets</span></a></li><li><a class="tocitem" href="#Declaring-metrics"><span>Declaring metrics</span></a></li><li><a class="tocitem" href="#Declaring-methodologies"><span>Declaring methodologies</span></a></li><li><a class="tocitem" href="#Structure-and-configuration"><span>Structure and configuration</span></a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../examples/mock2-memorythroughput/">Effective memory throughput</a></li><li><a class="tocitem" href="../examples/mock3-roofline/">Roofline</a></li><li><a class="tocitem" href="../examples/mock4-recursive/">Recursive Suite Generation</a></li></ul></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../limitations/">Limitations</a></li><li><a class="tocitem" href="../api/">API reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Macros</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Macros</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaPerf/PerfTest.jl/blob/master/docs/src/macros.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="PerfTest-macros-quick-reference"><a class="docs-heading-anchor" href="#PerfTest-macros-quick-reference">PerfTest macros quick reference</a><a id="PerfTest-macros-quick-reference-1"></a><a class="docs-heading-anchor-permalink" href="#PerfTest-macros-quick-reference" title="Permalink"></a></h1><p>The following are the main macros used to define performance test suites. These shall be always used inside a testset (see the [Test] package). Combining the different macros listed in this section gives the full extent of the package features.</p><h2 id="Declaring-test-targets"><a class="docs-heading-anchor" href="#Declaring-test-targets">Declaring test targets</a><a id="Declaring-test-targets-1"></a><a class="docs-heading-anchor-permalink" href="#Declaring-test-targets" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="PerfTest.@perftest"><a class="docstring-binding" href="#PerfTest.@perftest"><code>PerfTest.@perftest</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>This macro is used to signal that the wrapped expression is a performance test target, and therefore its performance will be sampled and then evaluated following the current suite configuration.</p><p>If the macro is evaluated it does not modify the target at all. The effects of the macro only show when the script is transformed into a performance testing suite.</p><p>This macro is sensitive to context since other adjacent macros can change how the target will be evaluated.</p><pre><code class="language-julia hljs">@perftest expression [parameters...]</code></pre><p>Run a performance test on a given target expression.</p><p><strong>Basic usage</strong></p><p>The simplest usage is to place <code>@perftest</code> in front of the expression you want to test:</p><pre><code class="language-julia hljs">julia&gt; @perftest sin(1)</code></pre><p><strong>Additional parameters</strong></p><p>You can pass the following keyword arguments to configure the execution process:</p><ul><li><p><strong><code>setup</code></strong>: An expression that is run once per sample before the benchmarked expression. The <code>setup</code> expression is run once per sample, and is not included in the timing results. Note that each sample can require multiple evaluations.</p></li><li><p><strong><code>teardown</code></strong>: An expression that is run once per sample after the benchmarked expression.</p></li><li><p><strong><code>samples</code></strong>: The number of samples to take. Execution will end if this many samples have been collected. Defaults to <code>10000</code>.</p></li><li><p><strong><code>seconds</code></strong>: The number of seconds budgeted for the benchmarking process. The trial will terminate if this time is exceeded (regardless of samples), but at least one sample will always be taken. In practice, actual runtime can overshoot the budget by the duration of a sample.</p></li><li><p><strong><code>evals</code></strong>: The number of evaluations per sample. For best results, this should be kept consistent between trials. A good guess for this value can be automatically set on a benchmark via <code>tune!</code>, but using <code>tune!</code> can be less consistent than setting <code>evals</code> manually (which bypasses tuning).</p></li><li><p><strong><code>gctrial</code></strong>: If <code>true</code>, run <code>gc()</code> before executing this benchmark&#39;s trial. Defaults to <code>true</code>.</p></li><li><p><strong><code>gcsample</code></strong>: If <code>true</code>, run <code>gc()</code> before each sample. Defaults to <code>false</code>.</p></li><li><p><strong><code>time_tolerance</code></strong>: The noise tolerance for the benchmark&#39;s time estimate, as a percentage. This is utilized after benchmark execution, when analyzing results. Defaults to <code>0.05</code>.</p></li></ul><p><strong>Examples</strong></p><p><strong>Basic performance test</strong></p><pre><code class="language-julia hljs"> @perftest sin(1)</code></pre><p><strong>With setup and teardown</strong></p><pre><code class="language-julia hljs"> @perftest sort!(data) setup=(data=rand(100)) teardown=(data=nothing)</code></pre><p><strong>With custom parameters</strong></p><pre><code class="language-julia hljs"># Run with a 3-second time budget
 @perftest sin(x) setup=(x=rand()) seconds=3

# Limit to 100 samples with 10 evaluations each
 @perftest myfunction(data) samples=100 evals=10

# Disable garbage collection before each sample
 @perftest allocating_function() gcsample=false gctrial=false</code></pre><p><strong>See Also</strong></p><ul><li><a href="https://juliaci.github.io/BenchmarkTools.jl/dev/">BenchmarkTools.jl Documentation</a> for more details on the underlying <code>@benchmark</code> macro and its parameters.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/perftest.jl#L23-L94">source</a></section></details></article><h2 id="Declaring-metrics"><a class="docs-heading-anchor" href="#Declaring-metrics">Declaring metrics</a><a id="Declaring-metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Declaring-metrics" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="PerfTest.@define_metric"><a class="docstring-binding" href="#PerfTest.@define_metric"><code>PerfTest.@define_metric</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>This macro is used to define a new custom metric.</p><p><strong>Arguments</strong></p><ul><li><code>name</code> : the name of the metric for identification purposes.</li><li><code>units</code> : the unit space that the metric values will be in.</li><li>formula block : an expression that returns a single value, which would be the metric value. The formula can have any julia expression inside and additionally some special symbols are supported. The formula may be evaluated several times, so its applied to every target in every test set or just once, if the formula is defined inside a test set, which makes it only applicable to it. NOTE: If there is the need of referring to a variable on a formula block, it first needs to be exported using the macro @export_vars, otherwise an error will occur.</li></ul><p><strong>Special symbols:</strong></p><ul><li><code>:median_time</code> : will be substituted by the median time the target took to execute in the benchmark.</li><li><code>:minimum_time</code>: will be substituted by the minimum time the target took to execute in the benchmark.</li><li><code>:ret_value</code> : will be substituted by the return value of the target.</li><li><code>:autoflop</code>: will be substituted by the FLOP count the target.</li><li><code>:printed_output</code> : will be substituted by the standard output stream of the target.</li><li><code>:iterator</code> : will be substituted by the current iterator value in a loop test set.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/customs.jl#L19-L34">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PerfTest.@auxiliary_metric"><a class="docstring-binding" href="#PerfTest.@auxiliary_metric"><code>PerfTest.@auxiliary_metric</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>Defines a custom metric for informational purposes that will not be used for testing but will be printed as output.</p><p><strong>Arguments</strong></p><ul><li><code>name</code> : the name of the metric for identification purposes.</li><li><code>units</code> : the unit space that the metric values will be in.</li><li>formula block : an expression that returns a single value, which would be the metric value. The formula can have any julia expression inside and additionally some special symbols are supported. The formula may be evaluated several times, so its applied to every target in every test set or just once, if the formula is defined inside a test set, which makes it only applicable to it.</li></ul><p><strong>Special symbols:</strong></p><ul><li><code>:median_time</code> : will be substituted by the median time the target took to execute in the benchmark.</li><li><code>:minimum_time</code>: will be substituted by the minimum time the target took to execute in the benchmark.</li><li><code>:ret_value</code> : will be substituted by the return value of the target.</li><li><code>:autoflop</code>: will be substituted by the FLOP count the target.</li><li><code>:printed_output</code> : will be substituted by the standard output stream of the target.</li><li><code>:iterator</code> : will be substituted by the current iterator value in a loop test set.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/customs.jl#L77-L92">source</a></section></details></article><h2 id="Declaring-methodologies"><a class="docs-heading-anchor" href="#Declaring-methodologies">Declaring methodologies</a><a id="Declaring-methodologies-1"></a><a class="docs-heading-anchor-permalink" href="#Declaring-methodologies" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="PerfTest.@perfcompare"><a class="docstring-binding" href="#PerfTest.@perfcompare"><code>PerfTest.@perfcompare</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>This macro is used to manually declare performance test conditions.</p><p><strong>Arguments</strong></p><ul><li>An expression that must result to a boolean when evaluated. Being true if the comparison leads to a succesful performance test. Special symbols can be used.</li></ul><p><strong>Special symbols:</strong></p><ul><li><code>:median_time</code> : will be substituted by the median time the target took to execute in the benchmark.</li><li><code>:minimum_time</code>: will be substituted by the minimum time the target took to execute in the benchmark.</li><li><code>:ret_value</code> : will be substituted by the return value of the target.</li><li><code>:autoflop</code>: will be substituted by the FLOP count the target.</li><li><code>:printed_output</code> : will be substituted by the standard output stream of the target.</li><li><code>:iterator</code> : will be substituted by the current iterator value in a loop test set.</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">    @perfcompare :median_time &lt; 0.05</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/perfcompare.jl#L9-L28">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PerfTest.@define_eff_memory_throughput"><a class="docstring-binding" href="#PerfTest.@define_eff_memory_throughput"><code>PerfTest.@define_eff_memory_throughput</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>This macro is used to define the memory bandwidth of a target in order to execute the effective memory thorughput methodology.</p><p><strong>Arguments</strong></p><ul><li>formula block : an expression that returns a single value, which would be the metric value. The formula can have any julia expression inside and additionally some special symbols are supported. The formula may be evaluated several times, so its applied to every target in every test set or just once, if the formula is defined inside a test set, which makes it only applicable to it.</li><li>ratio : the allowed minimum percentage over the maximum attainable that is allowed to pass the test, it can be a number or a Julia expression that evaluates to a number</li><li>mem<em>benchmark : which STREAM kernel benchmark to use (e.g :MEM</em>STREAM<em>COPY for transfer operations :MEM</em>STREAM_ADD for transfer and computing)</li><li>custom_benchmark : in case of using a custom benchmark, the symbol that identifies the chosen benchmark, (must have been defined before)</li></ul><p><strong>Special symbols:</strong></p><ul><li><code>:median_time</code> : will be substituted by the median time the target took to execute in the benchmark.</li><li><code>:minimum_time</code>: will be substituted by the minimum time the target took to execute in the benchmark.</li><li><code>:ret_value</code> : will be substituted by the return value of the target.</li><li><code>:autoflop</code>: will be substituted by the FLOP count the target.</li><li><code>:printed_output</code> : will be substituted by the standard output stream of the target.</li><li><code>:iterator</code> : will be substituted by the current iterator value in a loop test set.</li></ul><p><strong>Example:</strong></p><p>The following definition assumes that each execution of the target expression involves transacting 1000 bytes. Therefore the bandwith is 1000 / execution time.</p><pre><code class="language-julia hljs">@define_eff_memory_throughput begin
      1000 / :median_time
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/customs.jl#L119-L144">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PerfTest.@roofline"><a class="docstring-binding" href="#PerfTest.@roofline"><code>PerfTest.@roofline</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>This macro enables roofline modelling, if put just before a target declaration (<code>@perftest</code>) it will proceed to evaluate it using a roofline model.</p><p><strong>Mandatory arguments</strong></p><ul><li>formula block: the macro has to wrap a block that holds a formula to obtain the operational intensity of target algorithms.</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>cpu_peak</code> : a manual input value for the maximum attainable FLOPS, this will override the empirical runtime benchmark</li><li><code>membw_peak</code> : a manual input value for the maximum memory bandwith, this will override the empirical runtime benchmark</li><li><code>target_opint</code> : a desired operational intensity for the target, this will turn operational intensity into a test metric</li><li><code>actual_flops</code>: another formula that defines the actual performance of the test.</li><li><code>target_ratio</code> : the acceptable ratio between the actual performance and the projected performance from the roofline, this will turn actual performance into a test metric.</li></ul><p><strong>Special symbols:</strong></p><ul><li><code>:median_time</code> : will be substituted by the median time the target took to execute in the benchmark.</li><li><code>:minimum_time</code>: will be substituted by the minimum time the target took to execute in the benchmark.</li><li><code>:ret_value</code> : will be substituted by the return value of the target.</li><li><code>:autoflop</code>: will be substituted by the FLOP count the target.</li><li><code>:printed_output</code> : will be substituted by the standard output stream of the target.</li><li><code>:iterator</code> : will be substituted by the current iterator value in a loop test set.</li></ul><p>Any formula block specified in this macro supports these symbols.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">    @roofline actual_flops=:autoflop target_ratio=0.05 begin
        mem = ((:iterator + 1) * :iterator)
        :autoflop / mem
    end</code></pre><p>The code block defines operational intensity, whilst the other arguments define how to measure and compare the actual performance with the roofline performance. If the actual to projected performance ratio goes below the target, the test fails.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/roofline.jl#L50-L83">source</a></section></details></article><h2 id="Structure-and-configuration"><a class="docs-heading-anchor" href="#Structure-and-configuration">Structure and configuration</a><a id="Structure-and-configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Structure-and-configuration" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="PerfTest.@perftest_config"><a class="docstring-binding" href="#PerfTest.@perftest_config"><code>PerfTest.@perftest_config</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>Captures a set of configuration parameters that will override the default configuration. The parameters shall be written in TOML syntax, like a subset of the complete configuration (see config.toml generated by executing transform, or transform/configuration.jl for more information). Order is irrelevant. This macro shall be put as high as possible in the test file (code that is above will be transformed using the default configuration).</p><p><strong>Recursive transformation:</strong></p><p>This macro will set the new configuration keys for the current file and any other included files. If the included files have the macro as well, those macros will override the configuration locally for each file.</p><p><strong>Arguments</strong></p><ul><li>A String, with the TOML declaration of configuration keys</li></ul><p><strong>Example</strong></p><p>@perftest<em>config &quot; [roofline]   enabled = false [general]   max</em>saved_results = 1   recursive = false &quot;</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/configuration.jl#L9-L28">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PerfTest.@on_perftest_exec"><a class="docstring-binding" href="#PerfTest.@on_perftest_exec"><code>PerfTest.@on_perftest_exec</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>The expression given to this macro will only be executed in the generated suite, and will be deleted if the source code is executed as is.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/exec_ignore.jl#L9-L11">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PerfTest.@on_perftest_ignore"><a class="docstring-binding" href="#PerfTest.@on_perftest_ignore"><code>PerfTest.@on_perftest_ignore</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>The expression given to this macro will only be executed in the source code, and will be deleted in the generated performance test suite.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/exec_ignore.jl#L19-L21">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PerfTest.@export_vars"><a class="docstring-binding" href="#PerfTest.@export_vars"><code>PerfTest.@export_vars</code></a> — <span class="docstring-category">Macro</span></summary><section><div><p>@export_vars vars...</p><p>Exports the specified symbols –along with the values they hold at the moment of the calling– to the scope of metric definitions. In order to use any variable on the definition of a metric such variable needs to be exported with this macro.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPerf/PerfTest.jl/blob/42a86e908f5f37676e98e172f1abee181ac963cc/src/execution/macros/customs.jl#L153-L157">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../usage/">« Usage</a><a class="docs-footer-nextpage" href="../examples/">... »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Tuesday 3 February 2026 17:22">Tuesday 3 February 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
